{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243bd94e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ade828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/anaconda3/envs/silkworm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "from timm import create_model\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.cluster import MiniBatchKMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b361daf",
   "metadata": {},
   "source": [
    "## Globals\n",
    "Centralised configuration so it is easy to tweak hyper-parameters in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1951ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('./data/images')\n",
    "MODEL_DIR = Path('./models')\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 512x512 crops → lower batch for 6 GB GPUs; feel free to raise if you have 32 GB\n",
    "BATCH_SIZE  = 2\n",
    "IMG_SIZE    = 512          # high-res to catch thin worms\n",
    "NUM_EPOCHS  = 30\n",
    "LEARNING_RATE = 1e-5\n",
    "ACCUM_STEPS = 4             # keeps effective batch 8\n",
    "NUM_WORKERS = os.cpu_count() // 2 or 2\n",
    "\n",
    "NUM_CLUSTERS = 4            # background / leaf / worm\n",
    "SEED = 14\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e76c75a",
   "metadata": {},
   "source": [
    "## Utils\n",
    "Re-usable helpers (metrics, schedulers, visualisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3374299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"Simple context manager for timing code blocks\"\"\"\n",
    "    def __enter__(self):\n",
    "        self.start = datetime.now()\n",
    "        return self\n",
    "    def __exit__(self, *args):\n",
    "        self.duration = datetime.now() - self.start\n",
    "\n",
    "def cosine_decay(optimizer, epoch, total_epochs, base_lr):\n",
    "    \"\"\"Sets LR following a half-cosine schedule\"\"\"\n",
    "    lr = 0.5 * base_lr * (1 + np.cos(np.pi * epoch / total_epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def kmeans_labels(feats, k):\n",
    "    \"\"\"feats: (B, H, W, C) tensor on CUDA → returns (B, H, W) labels on CUDA\"\"\"\n",
    "    B, H, W, C = feats.shape\n",
    "    km = MiniBatchKMeans(k, n_init='auto', max_iter=30, batch_size=8192)\n",
    "    lbl = []\n",
    "    feats_np = feats.cpu().reshape(-1, C).numpy()\n",
    "    km.fit(feats_np)\n",
    "    pred = torch.from_numpy(km.labels_).to(feats.device)\n",
    "    return pred.view(B, H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca79209",
   "metadata": {},
   "source": [
    "## Data\n",
    "Minimal custom dataset that serves single-image batches for unsupervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e57bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as TF\n",
    "import random\n",
    "\n",
    "class SilkwormDataset(Dataset):\n",
    "    def __init__(self, root: Path):\n",
    "        self.img_paths = sorted([p for p in root.iterdir() \n",
    "                                  if p.suffix.lower() in {'.jpg', '.png'}])\n",
    "        self.resize_size = (IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "    def __len__(self): return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.img_paths[idx]).convert('RGB')\n",
    "\n",
    "        # --- random zoom keeps thin worms detailed ---\n",
    "        zoom_scale = random.uniform(0.8, 0.9)\n",
    "        w, h = img.size\n",
    "        nw, nh = int(w * zoom_scale), int(h * zoom_scale)\n",
    "        l, t = (w - nw) // 2, (h - nh) // 2\n",
    "        img = img.crop((l, t, l + nw, t + nh))\n",
    "        img = TF.pad(img,\n",
    "                     padding=(t, l, h - nh - t, w - nw - l),\n",
    "                     fill=(128, 128, 128))\n",
    "\n",
    "        # colour-shadow aug\n",
    "        img = TF.adjust_contrast(img,    random.uniform(1.2, 1.5))\n",
    "        img = TF.adjust_saturation(img,  random.uniform(1.2, 1.5))\n",
    "        img = TF.adjust_brightness(img,  random.uniform(0.9, 1.1))\n",
    "        img = TF.adjust_gamma(img,       random.uniform(0.9, 1.1))\n",
    "\n",
    "        img = TF.resize(img, self.resize_size)\n",
    "        ten = TF.to_tensor(img)\n",
    "        ten = TF.normalize(ten, mean=[0.485, 0.456, 0.406],\n",
    "                                std =[0.229, 0.224, 0.225])\n",
    "        return ten\n",
    "\n",
    "# Recreate dataloader\n",
    "dataloader = DataLoader(\n",
    "    SilkwormDataset(DATA_DIR),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f68b2f",
   "metadata": {},
   "source": [
    "## Network\n",
    "Frozen DINO ViT backbone + lightweight UNet decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighResFPN(nn.Module):\n",
    "    def __init__(self, in_ch: int, n_cls: int):\n",
    "        super().__init__()\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_ch, 256, 2, stride=2), nn.ReLU(True),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),           nn.ReLU(True)\n",
    "        )\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 2, stride=2),    nn.ReLU(True),\n",
    "            nn.Conv2d(64, 32, 3, padding=1),             nn.ReLU(True)\n",
    "        )\n",
    "        self.up3 = nn.Sequential(                             # extra stage for 1024 px\n",
    "            nn.ConvTranspose2d(32, 16, 2, stride=2),    nn.ReLU(True),\n",
    "            nn.Conv2d(16,  8, 3, padding=1),            nn.ReLU(True)\n",
    "        )\n",
    "        self.head = nn.Conv2d(8, n_cls, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        return self.head(x)\n",
    "\n",
    "class SegModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ViT-small, patch-8 DINO backbone\n",
    "        self.backbone = create_model(\n",
    "            \"vit_small_patch8_224.dino\", pretrained=True, num_classes=0)\n",
    "        # allow arbitrary image size (avoids 224×224 assert)\n",
    "        self.backbone.patch_embed.strict_img_size = False\n",
    "        for p in self.backbone.parameters(): p.requires_grad = False\n",
    "        self.decoder = HighResFPN(self.backbone.embed_dim, NUM_CLUSTERS)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        B = imgs.size(0)\n",
    "        tok = self.backbone.patch_embed(imgs)           # (B, N, D) with N = (H/8)*(W/8)\n",
    "        h = w = int(tok.size(1) ** 0.5)                 # for 1024 px ⇒ h=w=128\n",
    "        feat = tok.permute(0, 2, 1).reshape(B, -1, h, w)\n",
    "        return self.decoder(feat)                       # (B, C, 1024, 1024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b272acd3",
   "metadata": {},
   "source": [
    "## Train\n",
    "Generic training loop with mixed precision and runtime logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c99265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate network, move it to GPU, and create optimizer/criterion\n",
    "model = SegModel().to(DEVICE)\n",
    "\n",
    "scaler     = GradScaler()\n",
    "optimizer  = torch.optim.AdamW(model.decoder.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "\n",
    "with Timer() as t_total:\n",
    "    global_step = 0\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        pbar = tqdm(dataloader, desc=f'Epoch {epoch}/{NUM_EPOCHS}', leave=False)\n",
    "        for imgs in pbar:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            with autocast(device_type='cuda'):\n",
    "                logits = model(imgs)\n",
    "                # pseudo labels: k-means over tokens each batch\n",
    "                bs, _, h, w = logits.shape\n",
    "                flat_feats = logits.permute(0, 2, 3, 1).reshape(-1, NUM_CLUSTERS)\n",
    "                labels = torch.argmin(flat_feats, dim=1)  # placeholder pseudo-label\n",
    "                labels = labels.view(bs, h, w).to(DEVICE)\n",
    "                loss = criterion(logits, labels)\n",
    "                loss = loss / ACCUM_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            if (global_step + 1) % ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            epoch_loss += loss.item() * ACCUM_STEPS\n",
    "            global_step += 1\n",
    "            pbar.set_postfix(loss=epoch_loss / (global_step + 1))\n",
    "\n",
    "        cosine_decay(optimizer, epoch, NUM_EPOCHS, LEARNING_RATE)\n",
    "        torch.save(model.state_dict(), MODEL_DIR / f'seg_epoch{epoch:02d}.pth')\n",
    "\n",
    "print(f\"Training completed in {t_total.duration}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77322506",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Quickly verify class balance and log qualitative predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5de8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Get a small batch\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    imgs = next(iter(dataloader))[:4].to(DEVICE)                 # B×3×HxW\n",
    "    logits = model(imgs)                                          # B×C×h×w (low-res output from ViT grid)\n",
    "    masks = torch.argmax(logits, dim=1)                           # B×h×w (low-res masks)\n",
    "\n",
    "# Un-normalize images for display\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], device=DEVICE).view(3,1,1)\n",
    "std  = torch.tensor([0.229, 0.224, 0.225], device=DEVICE).view(3,1,1)\n",
    "imgs_disp = (imgs * std + mean).clamp(0,1).cpu().permute(0,2,3,1).numpy()    # B×H×W×3\n",
    "\n",
    "# Upsample masks to match input image resolution (nearest neighbor to preserve class labels)\n",
    "target_H, target_W = imgs.shape[2], imgs.shape[3]\n",
    "masks_upsampled = F.interpolate(\n",
    "    masks.unsqueeze(1).float(),                    # B×1×h×w\n",
    "    size=(target_H, target_W),\n",
    "    mode='nearest'\n",
    ").squeeze(1).cpu().numpy()                          # B×H×W (numpy)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "for i in range(4):\n",
    "    # Row 1: Original Image\n",
    "    axes[0,i].imshow(imgs_disp[i])\n",
    "    axes[0,i].set_title(\"Input\")\n",
    "    axes[0,i].axis(\"off\")\n",
    "    \n",
    "    # Row 2: Overlay\n",
    "    axes[1,i].imshow(imgs_disp[i])\n",
    "    axes[1,i].imshow(masks_upsampled[i], cmap=\"jet\", alpha=0.3)   # Use upsampled mask\n",
    "    axes[1,i].set_title(\"Overlay\")\n",
    "    axes[1,i].axis(\"off\")\n",
    "    \n",
    "    # Row 3: Mask Only\n",
    "    axes[2,i].imshow(masks_upsampled[i], cmap=\"tab20\")            # Also use upsampled mask\n",
    "    axes[2,i].set_title(\"Mask\")\n",
    "    axes[2,i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Silkworm)",
   "language": "python",
   "name": "silkworm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
